from .adapters.torch_op import flash_attention
